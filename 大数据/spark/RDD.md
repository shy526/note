# RDD编程

## RDD基础
- RDD就是一个不可变的分布式对象集合,
  - 每个RDD被分为多个区
    - 这些分区运行在集群中的不容节点上
- 包含Pyhon,java,Scala中任意类型的对象
  - 可以时自定义对象

- 创建RDD的两种方式
  1. 读取外部数据集
      - `SparkContext.parallelize()`
        - 需要将整个数据集放在一台机器上
        - 使用较少
      - `SparkContext.textFile()`
        - 读入一个文本文件
        - 常用

  2. 在驱动器程序里分发驱动器程序中对象集合

- RDD支持的两种操作
  1. 转化操作
      - 由一个RDD生成另一个新的RDD
      - spark会使用谱系图记录不同的RDD之间的依赖关系
      - `map()`,`filter()`
  2. 行动操作
      - 把结果返回到驱动程序中
      - 存储到外部存储系统中
      - `count()`,`filter()`
        - `filter()`:会返回一个新的RDD,原RDD不受影响
        - `collect()`:获取整个RDD数据
          - 整个数据集能在单台机器的内存放的下时才能使用
            - 不能用在大规模数据集上
      - 每一次行动操作都会重新计算一次
        - 应该避免这种低效操作
      - 可以将中间结果持久化
  - 两种操作的区别
    - spark计算RDD的方式不同
      - 惰性计算
        - 只有在行动操作中用到才会真正计算
        - 默认情况
          - 如果想重用一个RDD
            - `RDD.persist()`方法把RDD缓冲下来
              - 缓存到内存中
    - 转化操作只会返回RDD类型而行动操作则为其他类型
  - 保存RDD数据失败时
    - spark利用这种特性重算丢掉的分区

## Spark程序工作流程
1. 从外部数据创建输入RDD
2. 使用诸如`filter()`这样的转化操作对RDD进行转化,以定义新的RDD
3. 告诉Spark对需要被重用的中间结果RDD进行`persist()`操作
4. 使用行动操作来触发一次并行计算,spark会对计算进行优化并计算

## spark中传递函数
- 函数需要作为实现了 `org.apache.spark.api.function`包中的任意函数接口的对象传递
  - 根据不同的返回值定义不同的接口

- 基本的java函数接口

|        函数名        |      实现方法       |                              用途                              |
|:--------------------:|:-------------------:|:--------------------------------------------------------------:|
|    Function<T,R>     |      R call(T)      |   接收一个输入值并返回一个输出值,用于`map()``filter()`等操作   |
|    Function2<T,R>    |    R call(T1,T2)    | 接收连个输入值并返回一个输出值,用于`aggregate()``fold()`等操作 |
| FlatMapFunction<T,R> | Iterable<R> call(T) |     接收一个输入值并返回人一个输入值,用于`flatmap()`等操作     |
